---
title: "Practical Machine Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown



```{r packages}
setwd("C:\\Users\\Nastya\\Documents\\Coursera\\Practical Machine Learning")

library(caret)
library(rpart.plot)
library(pgmm)
library(gbm)
library(rpart)
library(rattle)

# create dataframes for the training and testing data

training<-read.csv(file="pml-training.csv", header=TRUE, sep=",")
dim(training)
testing<-read.csv(file="pml-testing.csv",header=TRUE,sep=",")
dim(testing)
str(training)
summary(training)
```

Before using the machine learning techniques, it is important to consider only informative variables in the model. The total number of variables except the response variable that are available in the training dataset is equal to 159. However, there exist variables that have little predictive power due to many missing observations. These variables should be removed from the analysis. Additionally, we remove the variables that correspond to the infromation about the participants of the experiment and the timestamps as well as varables that have yero variability.

```{r, echo=FALSE}
# This part of the code sets the index for the variables that have equall or more than 90% of missing values (NA or blank) in the training set 

indexMissing <- which(colSums(is.na(training)|training=="")>=0.9*dim(training)[1]) 
trainClean <- training[,-indexMissing]
# Now we remove first 7 variable since they are not informative for the prediction of the variable classe
trainClean <- trainClean[,-c(1:7)]
# Check whether there are variables with almost zero variance and remove them
nzv<-nearZeroVar(trainClean,saveMetrics=TRUE)
trainClean <- trainClean[,nzv$nzv==FALSE]
dim(trainClean)


# We do the same for the testing set provided for the evaluation of the models
 
validationClean <- testing[,-indexMissing]
validationClean <- validationClean[,-c(1:7)]
dim(validationClean)
```
The cleaned traning set contains 53 variables. On the next step we create a random partition of the training set, so we have an independent validation set.

```{r, echo=FALSE}

# Here we create a partition of the traning data set 
set.seed(333333)
trainIndex <- createDataPartition(trainClean$classe, p=0.7, list=FALSE)
trainFinal <- trainClean[trainIndex,]
validationFinal <- trainClean[-trainIndex,]
# Now we partition the rest of the data in the training and testing set
inTrain<-createDataPartition(trainFinal$classe, p=0.7, list=FALSE)
trainingFinal<-trainFinal[inTrain,]
testingFinal<-trainFinal[-inTrain,]

dim(trainingFinal)
dim(testingFinal)
dim(validationFinal)

```

After cleaning the data, in order to chose an appropriate variables for the model and prediction function we will use K-fold cross validation technique with number of folds equal to 10 (what is a common choise among practitioners) in order to avoid overfitting. We will compare the performance of several types of the machine learning techniques: classification tree, random forest, generalized linear model and gradient boosting method.
# Classification tree
```{r, echo=FALSE}
# use trainControl to perform cross-validation with 10 folds while building the models in the training set

trCtrl <- trainControl(method="cv", number=10)

# build a classification tree using the training set
mod_ct<-train(classe~., data=trainingFinal, method="rpart", trControl=trCtrl)

# We can plor the classification tree by using the following command
fancyRpartPlot(mod_ct$finalModel)

# then we use the final model to predict values on the training set
trainPred_ct<-predict(mod_ct,testingFinal)

# get the testing errors from the confusion matrix
CM_ct<- confusionMatrix(trainPred_ct,testingFinal$classe)
CM_ct$table 
# get the overall accuracy of the prediction on the testing set and out-of-sample error
accuracy_ct<-CM_ct$overall[1]
testing_out_of_sample_error_ct <- 1-accuracy_ct
```

The error of the classification of the method is pretty high (__`r testing_out_of_sample_error_ct`__). 

# Random forest
```{r, echo=FALSE}
# train a random forest 

mod_rf <- train(classe~., data=trainingFinal, method="rf", trControl=trCtrl, verbose=FALSE)

print(mod_rf)

# predict values of the testing set

trainPred_rf <- predict(mod_rf,newdata=testingFinal)

CM_rf <- confusionMatrix(trainPred_rf,testingFinal$classe)

# display confusion matrix, model accuracy and overall error rate
CM_rf$table
accuracy_rf<-CM_rf$overall[1]
testing_out_of_sample_error_rf <- 1 - accuracy_rf
```
Next candidate method is boosting with trees.

# Boosting
```{r, echo=FALSE}
mod_gbm <- train(classe~., data=trainingFinal, method="gbm", trControl=trControl, verbose=FALSE)
print(model_gbm)

# predict values of the testing set

trainPred_gbm <- predict(mod_gbm,newdata=testingFinal)

CM_gbm <- confusionMatrix(trainPred_gbm,testingFinal$classe)

# display confusion matrix, model accuracy and overall error rate

CM_gbm$table
accuracy_gbm<-CM_gbm$overall[1]
testing_out_of_sample_error_gbm <- 1 - accuracy_gbm

plot(mod_gbm)
```
# Generalized linear model
```{r, echo=FALSE}
mod_glm <- train(classe~., data=trainingFinal, method="glm", trControl=trControl)
print(model_glm)

# predict values of the testing set

trainPred_glm <- predict(mod_glm,newdata=testingFinal)

CM_glm <- confusionMatrix(trainPred_glm,testingFinal$classe)

# display confusion matrix, model accuracy and overall error rate

CM_glm$table
accuracy_glm<-CM_glm$overall[1]
testing_out_of_sample_error_glm <- 1 - accuracy_glm

# see the relative performance of rendom forest and glm model in the prediction exercise for the test set
qplot(trainPred_glm,trainPred_rf,colour=classe,data=testingFinal)
```
We can as well combine the predictors to get better performance. We will combine the random forest with boosting and the glm model on the training set and see how the accuracy of the predictions changes.

```{r, echo=FALSE}
# create the data frame with the forecasts on the training set
predDF <- data.frame(trainPred_rf, trainPred_gbm, trainPred_glm, classe = testingFinal$classe)

# train the model and evaluate predictions
combModFit <- train(classe ~ ., method = "rf", data = predDF)
combPred <- predict(combModFit, predDF)

# display confusion matrix, model accuracy and overall error rate

CM_mix<-confusionMatrix(combPred, testing$diagnosis)$overall[1]
CM_mix$table
accuracy_mix<-CM_mix$overall[1]
testing_out_of_sample_error_mix <- 1 - accuracy_mix

```
Based on the analysis of the training set, we coose the mixed model for the prediction exercise. In order to get representative out-of-sample errors we need to evaluate the models on the validation set.

## Evaluate the model and out-of-sample errors of the chosen models on the validation set

```{r, echo=FALSE}
predV_rf<-predict(mod_rf,validationFinal)
predV_gbm<-predict(mod_gbm,validationFinal)
predV_glm<-predict(mod_glm,validationFinal)

predVDF<-data.frame(pred1=predV_rf, pred2=predV_gbm, pred3=predV_glm)
combPredV<-predict(combModFit,predVDF)

# display confusion matrix, model accuracy and overall error rate

CMV_mix<-confusionMatrix(combPredV, validationFinal$classe)$overall[1]
CMV_mix$table
out_of_sample_error_mix <- 1 - CMV_mix$overall[1]
```

# Prediction for the 20 test cases
Here are the predictions of the 20 test cases based on the models chosen above.
```{r, echo=FALSE}
p_rf<-predict(mod_rf,validationClean)
p_gbm<-predict(mod_gbm,validationClean)
p_glm<-predict(mod_glm,validationClean)

pDF<-data.frame(pred1=p_rf, pred2=p_gbm, pred3=p_glm)
combP<-predict(combModFit,pDF)


p_rf
p_gbm
p_glm
combP
```
